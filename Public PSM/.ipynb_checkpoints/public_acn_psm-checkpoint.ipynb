{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sqlalchemy import column\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "import logging\n",
    "import time, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b50e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set time zone\n",
    "os.environ['TZ'] = 'US/Pacific'\n",
    "\n",
    "class xgb_psm():\n",
    "    \"\"\"Custom Propensity Scoring Class using XGBoost.\n",
    "    Authors: Carl Sharpe, Greg Ridgeway, Loren Atherley, Emma Heo\n",
    "    This class uses XGboost to generate propensity scores for binary separable \n",
    "    data with multiple categorical and/or continuous variables. Created in \n",
    "    partnership with Seattle Police Department, the initial goal is to identify the \n",
    "    propensity score for white/non-white subjects who interact with the police.\n",
    "\n",
    "    key variables:\n",
    "    self.data: the dataframe loaded into the object\n",
    "    self.X: dataframe of X features\n",
    "    self.y: dataframe of y labels\n",
    "    self.full_processor: the full sklearn processor with categorical and \n",
    "                         continuous variables\n",
    "    self.evaluated_data: data with propensity scores, probabilities, and \n",
    "                         predictions\n",
    "    XGBoost Classifier:\n",
    "    self.xgb_cl\n",
    "\n",
    "    Order of Execution:\n",
    "    1. evaluator = xgb_psm(): Construct object \n",
    "    2. evaluator.load_dataframe(dataframe)\n",
    "    3. evaluator.create_labels(label_col, label_val)\n",
    "    4. evaluator.separate_X_y(drop_cols, y_col)\n",
    "    5. evaluator.create_pipeline()\n",
    "    6. evaluator.process_X_y()\n",
    "    7. evaluator.fit_xgb()\n",
    "    8. evaluator.predict()\n",
    "    9. evaluator.generate_weights()\n",
    "    \n",
    "    To-Do:\n",
    "    - Add more error handling around badly ordered operations will give generic errors.\n",
    "    - Add more debugging.\n",
    "    - Enable more configuration for the classifier constructor.\n",
    "    - Enable more customization for fitting data to support test/train splits.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, verbosity=0):\n",
    "        \"\"\"Constructs the class, sets the verbosity (default 0). Uses Pythons'\n",
    "        native logging capability\n",
    "        verbosity = 1 <-- Warn\n",
    "        verbosity = 2 <-- Info\n",
    "        verbosity = 3 <-- Debug\n",
    "        \"\"\"\n",
    "        logging.basicConfig(format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "                            datefmt=\"%m/%d/%Y %I:%M:%S %p %Z\")\n",
    "        self.logger = logging.getLogger(\"Jarvis\")\n",
    "        if verbosity == 3:\n",
    "            self.logger.setLevel(logging.DEBUG)\n",
    "            self.logger.debug(\"Verbosity set to DEBUG\")\n",
    "        if verbosity == 2:\n",
    "            self.logger.setLevel(logging.INFO)\n",
    "            self.logger.info(\"Verbosity set to INFO\")\n",
    "        if verbosity == 1:\n",
    "            self.logger.setLevel(logging.WARN)\n",
    "            self.logger.warn(\"Verbosity set to WARN\")\n",
    "        if verbosity == 0:\n",
    "            self.logger.setLevel(logging.ERROR)\n",
    "        self.data = \"\"\n",
    "\n",
    "    def load_dataframe(self, df):\n",
    "        \"\"\"\n",
    "        loads the dataframe to be evaluated into self.data\n",
    "        \"\"\"\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            self.data = df\n",
    "        else:\n",
    "            print(\n",
    "                \"Passed variable not a dataframe... Please pass a dataframe.\")\n",
    "\n",
    "    def assign_label(self, input_label, value):\n",
    "        \"\"\"Method that takes the input label and assigns a 0 if match and \n",
    "        1 if non-match. Designed for performance to work with dataframe.map() and \n",
    "        lambda. See create_labels() for more info.\"\"\"\n",
    "        if input_label == value:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def create_labels(self, source_column, label_value, label_col):\n",
    "        \"\"\"Creates the labels for the test and control groups. Used in conjunction \n",
    "        with assign_label()\"\"\"\n",
    "        self.data[label_col] = self.data[source_column].map(\n",
    "            lambda x: self.assign_label(x, label_value))\n",
    "\n",
    "    def separate_X_y(self, X_drop_cols, y_col):\n",
    "        \"\"\"Separated X and y into discreet dataframes. Drops X_cols as requested.\"\"\"\n",
    "        columns = X_drop_cols\n",
    "        columns.append(y_col)\n",
    "        columns = list(set(columns))\n",
    "        result = [item for item in columns if (item not in self.data.columns)]\n",
    "        if len(result) > 0:\n",
    "            print(\"The following column name(s) were not found:\")\n",
    "            for item in result:\n",
    "                print(item)\n",
    "            print(\n",
    "                f\"If one of these colums is {y_col} then try running make_labels before running this\"\n",
    "            )\n",
    "        else:\n",
    "            self.X = self.data.drop(columns=X_drop_cols)\n",
    "            self.y = self.data[y_col]\n",
    "\n",
    "    def create_pipeline(self):\n",
    "        \"\"\"Creates full processor by interrogating the X dataframe types for numeric and non-numeric\n",
    "        datatypes to create categorical and continuous encoders.\"\"\"\n",
    "        if isinstance(self.data, pd.DataFrame):\n",
    "            self.categorical_pipeline = Pipeline(steps=[\n",
    "                (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"oh-encode\",\n",
    "                 OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),\n",
    "            ])\n",
    "            self.numeric_pipeline = Pipeline(\n",
    "                steps=[(\"impute\", SimpleImputer(\n",
    "                    strategy=\"mean\")), (\"scale\", StandardScaler())])\n",
    "\n",
    "            cat_cols = self.X.select_dtypes(exclude=\"number\").columns\n",
    "            num_cols = self.X.select_dtypes(include=\"number\").columns\n",
    "\n",
    "            self.full_processor = ColumnTransformer(transformers=[\n",
    "                (\"numeric\", self.numeric_pipeline, num_cols),\n",
    "                (\"categorical\", self.categorical_pipeline, cat_cols),\n",
    "            ])\n",
    "        else:\n",
    "            print(\n",
    "                \"Object does not contain a dataframe, please use load_dataframe to load a dataframe\"\n",
    "            )\n",
    "\n",
    "    def create_xgb(self, verbosity=0):\n",
    "        \"\"\"Constructs the XGBoost classifier. Parameters provided by Greg Ridgeway.\"\"\"\n",
    "        self.xgb_cl = xgb.XGBClassifier(max_depth=2,\n",
    "                                        n_estimators=3000,\n",
    "                                        objective=\"binary:logistic\",\n",
    "                                        learning_rate=0.01,\n",
    "                                        eval_metric='logloss',\n",
    "                                        verbosity=verbosity,\n",
    "                                        use_label_encoder=False)\n",
    "\n",
    "    def process_X_y(self):\n",
    "        \"\"\"Converts X and y dataframes to matrices to prepare them for fitting into the classifier\"\"\"\n",
    "        self.X_processed = self.full_processor.fit_transform(self.X)\n",
    "        self.y_processed = SimpleImputer(\n",
    "            strategy=\"most_frequent\").fit_transform(\n",
    "                self.y.values.reshape(-1, 1))\n",
    "\n",
    "    def fit_xgb(self):\n",
    "        \"\"\"Fits the classifier on X_processes and y_processed\"\"\"\n",
    "        self.xgb_cl.fit(self.X_processed, self.y_processed, verbose=2)\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"Runs the prediction and probability calculations for the fitted classifier\n",
    "        against X_processed and y_processed and stores them in:\n",
    "        - evaluated_data: the original dataframe, and\n",
    "        - prediction the binary prediction\n",
    "        - probability_0: the probability that the prediction is 0\n",
    "        - probability_1: the probability that the prediction is 1\"\"\"\n",
    "\n",
    "        self.predictions = self.xgb_cl.predict(self.X_processed)\n",
    "        self.probabilities = self.xgb_cl.predict_proba(self.X_processed)\n",
    "        self.evaluated_data = self.data.copy()\n",
    "        self.evaluated_data['prediction'] = self.predictions\n",
    "        self.evaluated_data['probability_0'] = self.probabilities[:,\n",
    "                                                                  0].tolist()\n",
    "        self.evaluated_data['probability_1'] = self.probabilities[:,\n",
    "                                                                  -1].tolist()\n",
    "\n",
    "    def generate_weights(self, label_col):\n",
    "        self.evaluated_data['weight'] = self.evaluated_data.apply(\n",
    "            lambda x: 1 if x[label_col] == 1 else x['probability_1']/(1-x['probability_1']), axis = 1\n",
    "        )\n",
    "\n",
    "    def extract_model_weights(self):\n",
    "        \"\"\"Extracts the model weights from the fitted classifier and converts \n",
    "        them back into the original label names for human readability.\"\"\"\n",
    "\n",
    "        features = self.X.columns\n",
    "        feature_map = self.full_processor.transformers_[1][1][\n",
    "            'oh-encode'].get_feature_names(features).tolist()\n",
    "        self.xgb_cl.get_booster().feature_names = feature_map\n",
    "        feature_important = self.xgb_cl.get_booster().get_score(\n",
    "            importance_type='weight')\n",
    "        keys = list(feature_important.keys())\n",
    "        values = list(feature_important.values())\n",
    "        self.model_weights = pd.DataFrame(data=values,\n",
    "                                          index=keys,\n",
    "                                          columns=[\"weight\"]).sort_values(\n",
    "                                              by=\"weight\", ascending=True)\n",
    "    \n",
    "    def absolute_difference(self, label_col):\n",
    "        \"\"\"\n",
    "        The absolute difference is calculated at a per row basis per exsperiment. \n",
    "        The sum because it should be summed over each exsperiment?\n",
    "\n",
    "        Function return the absolute difference:\n",
    "\n",
    "        mean_control = sum((1-label)*weight*frisk) / sum((1-label)*weight)\n",
    "        mean_treat = sum(label*frisk) / sum(label)\n",
    "        \n",
    "        absolute_difference = mean_treat - mean_control\n",
    "        \n",
    "        The sum is done per experiment. \n",
    "        \"\"\"\n",
    "        frisk_conversion = {'Y': 1,'N': 0}\n",
    "        frisk = self.evaluated_data[\"Frisk Flag\"].map(frisk_conversion)\n",
    "        weight = self.evaluated_data['weight']\n",
    "        label = self.evaluated_data[label_col]\n",
    "\n",
    "        mean_control = (1-label)*weight*frisk / (1-label)*weight    \n",
    "        mean_treat = (label*frisk) / label\n",
    "\n",
    "        #     (0*1)/0 results in NAN value  \n",
    "        self.evaluated_data['mean_control'] = mean_control.fillna(0)\n",
    "        self.evaluated_data['mean_treat'] = mean_treat.fillna(0)\n",
    "\n",
    "        self.evaluated_data[\"mean_control_sum\"] = self.evaluated_data['mean_control'].sum()\n",
    "        self.evaluated_data[\"mean_treat_sum\"] = self.evaluated_data['mean_treat'].sum()\n",
    "        self.evaluated_data[\"absolute_difference\"] = self.evaluated_data[\"mean_treat_sum\"] - self.evaluated_data[\"mean_control_sum\"]\n",
    "\n",
    "    def just_send_it(self, df, control_col, control_val, drop_cols, label_col):\n",
    "        \"\"\"Automatically runs all the required methods to prep the data, fit the \n",
    "        model, calculate the predictions and the probabilities, and the feature\n",
    "        weights.\"\"\"\n",
    "\n",
    "        self.logger.info(\"Loading Dataframe\")\n",
    "        self.load_dataframe(df)\n",
    "        self.logger.info(\"Creating Labels\")\n",
    "        self.create_labels(control_col, control_val, label_col)\n",
    "        self.logger.info(\"Separating X and y\")\n",
    "        self.separate_X_y(drop_cols, label_col)\n",
    "        self.logger.info(\"Creating Pipelines\")\n",
    "        self.create_pipeline()\n",
    "        self.logger.info(\"Creating XGBoost Classifier\")\n",
    "        self.create_xgb()\n",
    "        self.logger.info(\"Converting X and y to matrices\")\n",
    "        self.process_X_y()\n",
    "        self.logger.info(\"Fitting Classifier on X and y\")\n",
    "        self.fit_xgb()\n",
    "        self.logger.info(\"Calculating Predictions and Probabilities\")\n",
    "        self.predict()\n",
    "        self.logger.info(\"Generating Weights\")\n",
    "        self.generate_weights(label_col)\n",
    "        self.logger.info(\"Calculating absolute difference\")\n",
    "        self.absolute_difference(label_col)\n",
    "\n",
    "\n",
    "def evaluate_all_stops(df):\n",
    "    \"\"\"Function tightly coupled to the xgb_psm class that enables bulk processing\n",
    "    of all records in a given dataframe. Currently scoped to Precinct, Year, Month \n",
    "    intervals. \n",
    "\n",
    "    Args:\n",
    "        df (pandas dataframe): the dataframe to be evaluated\n",
    "\n",
    "    Returns:\n",
    "        summary_df: a summary dataframe of the experiments that include:\n",
    "                    - Precinct\n",
    "                    - Year\n",
    "                    - Month\n",
    "                    - Total Records\n",
    "                    - Attempted to Assess\n",
    "                    - Label 0 Records\n",
    "                    - Label 1 Records\n",
    "                    - Run time\n",
    "        propensity_df: a dataframe that includes the original input dataframe and\n",
    "                       adds the following columns:\n",
    "                       - Prediction\n",
    "                       - Probability 0\n",
    "                       - Probability 1\n",
    "                       - PSM Weight\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the summary_df dataframe\n",
    "    summary_df = pd.DataFrame({\n",
    "        'total_records': pd.Series(dtype='int'),\n",
    "        'attempted_to_assess': pd.Series(dtype='str'),\n",
    "        'label_0_records_count': pd.Series(dtype='int'),\n",
    "        'label_1_records_count': pd.Series(dtype='int'),\n",
    "    })\n",
    "\n",
    "    propensity_df = pd.DataFrame()\n",
    "\n",
    "    # Setting up the default variables.\n",
    "\n",
    "    control_col = 'Subject Perceived Race'\n",
    "    control_val = 'White'\n",
    "    drop_cols = [\n",
    "        'Subject Perceived Race', 'GO / SC Num', 'Terry Stop ID',\n",
    "        'Subject ID'\n",
    "    ]\n",
    "    label_col = 'label'\n",
    "    print(f\"Found {len(df)} potential records to evaluate...\")\n",
    "\n",
    "    precincts = [\"NORTH\", \"EAST\", \"SOUTH\", \"WEST\", \"SOUTHWEST\"]\n",
    "    # Filter on Precinct\n",
    "    time_assessed = datetime.today()\n",
    "    for precinct in precincts:\n",
    "        precinct_df = df[df.precinct == precinct]\n",
    "        years = precinct_df.observation_year_d.unique()\n",
    "        # Filter on year\n",
    "        for year in years:\n",
    "            year_df = precinct_df[precinct_df.observation_year_d == year]\n",
    "            months = year_df.observation_month_d.unique()\n",
    "            # Filter on Month\n",
    "            for month in months:\n",
    "                month_df = year_df[year_df.observation_month_d == month]\n",
    "                # Check if it has more than 10 records\n",
    "                if len(month_df) > 10:\n",
    "                    eval_flag = True\n",
    "                    start_time = datetime.now()\n",
    "                    evaluator = xgb_psm(verbosity=0)\n",
    "                    \n",
    "                    evaluator.just_send_it(month_df,\n",
    "                                           control_col=control_col,\n",
    "                                           control_val=control_val,\n",
    "                                           drop_cols=drop_cols,\n",
    "                                           label_col=label_col)\n",
    "                    \n",
    "                    run_time = datetime.now() - start_time\n",
    "                    high_prop = evaluator.evaluated_data.weight.max()\n",
    "                    low_prop = evaluator.evaluated_data.weight.min()\n",
    "                    mean_prop = evaluator.evaluated_data.weight.mean()\n",
    "                    assessed = \"Yes\"\n",
    "                else:\n",
    "                    # If you skip the evaluation of records, write out the\n",
    "                    # flags appropriately\n",
    "                    eval_flag = False\n",
    "                    assessed = \"No\"\n",
    "                    run_time = None\n",
    "                    high_prop = None \n",
    "                    low_prop = None\n",
    "                    mean_prop = None\n",
    "\n",
    "                num_0 = month_df[month_df['Subject Perceived Race'] == \"White\"].shape[0]\n",
    "                num_1 = month_df[month_df['Subject Perceived Race'] != \"White\"].shape[0]\n",
    "                row = {\n",
    "                    \"total_records\": len(month_df),\n",
    "                    \"attempted_to_assess\": assessed,\n",
    "                    \"label_0_records_count\": num_0,\n",
    "                    \"label_1_records_count\": num_1,\n",
    "                    \"highest_propensity_pred\": high_prop,\n",
    "                    \"lowest_propensity_pred\": low_prop,\n",
    "                    \"mean_propensity_pred\": mean_prop,\n",
    "                    \"run_time\": str(run_time),\n",
    "                    \"run_timestamp\": time_assessed\n",
    "                }\n",
    "                summary_df = summary_df.append(row, ignore_index=True)\n",
    "                if eval_flag:\n",
    "                    propensity_df = propensity_df.append(\n",
    "                        evaluator.evaluated_data\n",
    "                    )\n",
    "                    propensity_df['run_timestamp'] = time_assessed\n",
    "                else:\n",
    "                    pass\n",
    "    return eval_flag, summary_df, propensity_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
