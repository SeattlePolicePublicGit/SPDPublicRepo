{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f0e876",
   "metadata": {},
   "source": [
    "Due to computational cost, at SPD we have separate jobs for feature engineering for training and feature engineering for inference. See the documentation for more information.\n",
    "\n",
    "The following main packages/modules are used:\n",
    "- pandas\n",
    "- numpy\n",
    "- NLTLK\n",
    "- scikit-learn\n",
    "- scipy\n",
    "- joblib\n",
    "\n",
    "We also use functions from the NLP_Preprocessing file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ada61093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "\n",
    "import sagemaker_pyspark\n",
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "#Data Manipulation\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#NLP Preprocessing Functions\n",
    "from NLP_PreProcessing import removeStopWords, removeFeatures, lemmatize\n",
    "\n",
    "#Model Creation \n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1faa7b",
   "metadata": {},
   "source": [
    "## 1. Load new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b1cffd",
   "metadata": {},
   "source": [
    "Unlike the report-level dataset for monthly training, the data source for daily inferences is a dataset with unique offenses per row. The flattening/pivoting of the offense dataset is computationally extensive; as such, the report-level dataset for training is created once a month exclusively for training. A dummy dataset of ~40 unique offenses was created for demonstration purposes. A more detailed description of the features in the dataset is included in 'feature_engineering_inference_documentation.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ddd827af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read incident offense dataset\n",
    "demographics = pd.read_csv('incident_offense.csv', usecols=['report_id','reporting_event_number','subject_ethnicity',\n",
    "                                                            'subject_gender','subject_personid','offense_id', 'offense_code_id',\n",
    "                                                            'subject_race','subject_age', 'victim_age', 'victim_ethnicity',\n",
    "                                                            'victim_gender', 'victim_personid', 'victim_race', 'beat',\n",
    "                                                            'crime_description','precinct', 'event_start_date', 'approval_status', 'report_submitted_date','report_ucr_approved_by'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "099e2069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure date fields are in date format\n",
    "demographics['event_start_date'] = pd.to_datetime(demographics['event_start_date'])\n",
    "demographics['report_submitted_date'] = pd.to_datetime(demographics['report_submitted_date'])\n",
    "\n",
    "demographics = demographics.sort_values(by='report_submitted_date', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cddd5e",
   "metadata": {},
   "source": [
    "We add a condition checking if a table with processed reports exist (if there is not one, the script assumes this is the first time the script is deployed).This code script runs daily. However, we might want to pass more than 24 hrs of reports the first time the script runs. We look at 15 days worth of reports, but this can be adjusted by the researcher as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "13506202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that final_reports.csv exists:\n",
    "if os.path.isfile('final_reports.csv'):\n",
    "    \n",
    "    #read final_reports table\n",
    "    fe_last = pd.read_csv(\"final_reports.csv\")\n",
    "    fe_last['report_submitted_date'] = pd.to_datetime(fe_last['report_submitted_date'])\n",
    "    \n",
    "    # 1) Look for latest date in output table and take all reports at that date and later\n",
    "    # (in case some reports with the last date were not processed) -->table gets replaced every time it runs:\n",
    "    latest_date = sorted(fe_last['report_submitted_date'].unique().tolist(), reverse=True)[0]\n",
    "    mask = demographics['report_submitted_date'] >= latest_date\n",
    "    filtered_demo = demographics[mask]\n",
    "    \n",
    "    # 2) Filter out reports already in feature engineering\n",
    "    recent_reports = set(fe_last['reporting_event_number'].unique().tolist())\n",
    "    filtered_demo = filtered_demo[~filtered_demo['reporting_event_number'].isin(recent_reports)]\n",
    "    \n",
    "    # 3) Filter reports in 'draft' status (should not be any)\n",
    "    filtered_demo = filtered_demo[filtered_demo['approval_status'] != 'Draft']\n",
    "    \n",
    "    # 4) Filter reports that have been updated by Tory (bias crimes unit research analyst):\n",
    "    # if using mark43 table: filtered_demo = filtered_demo[filtered_demo['updated_by'] != 'TORY WHITE']\n",
    "    filtered_demo = filtered_demo[filtered_demo['report_ucr_approved_by'] != '9601']\n",
    "    \n",
    "else: \n",
    "    # 1) Filter reports in 'draft' status\n",
    "    filtered_demo = demographics[demographics['approval_status'] != 'Draft']\n",
    "    \n",
    "    # 2) Filter reports that have been updated by Tory (bias crimes unit research analyst):\n",
    "    filtered_demo = filtered_demo[filtered_demo['report_ucr_approved_by'] != '9601']\n",
    "    \n",
    "    # 3) Look back over the past 15 days only:\n",
    "    fifteen_days = datetime.now() - pd.DateOffset(days=15)\n",
    "    filtered_demo = filtered_demo[filtered_demo['report_submitted_date'] >= fifteen_days]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8d42fd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = filtered_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5e0566",
   "metadata": {},
   "source": [
    "We add an if statement to check if the dataframe is empty in case no new reports (not yet processed) exist. The rest of the code cells only run if this is not true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a205a8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No error, continue running\n"
     ]
    }
   ],
   "source": [
    "if df.empty:\n",
    "    raise ValueError(\"Dataframe is empty.\")\n",
    "    \n",
    "print(\"No error, continue running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e56ee38",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da376fe",
   "metadata": {},
   "source": [
    "In order to order offenses, we create a rank of offenses related to events with bias elements, which includes deprecated offense codes present in historical data, RCW and SMC offense codes, and SPD-specific codes used for reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b2735a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of ranked crime descriptions\n",
    "offense_code_ranks = {\"RCW - 9A.36.080 | HATE CRIME OFFENSE\": 1,\n",
    "    \"RCW - 9A.36.080 | MALICIOUS HARASSMENT\": 2,\n",
    "    \"SMC - - 12A.06.115 | MALICIOUS HARASSMENT\": 3,\n",
    "    \"Incident Contains Bias Elements -- NO CRIME\": 4,\n",
    "    \"Offense Contains Bias Elements -- CRIME\": 5,\n",
    "    \"X91 | MALICIOUS HARASSMENT\": 6,\n",
    "    \"X92 | BIAS INCIDENT\": 7}\n",
    "\n",
    "#Create rank for offenses (9 for '-' in description)\n",
    "df[\"offense_rank\"] = df[\"crime_description\"].apply(lambda x: offense_code_ranks.get(x, 9) if x == '-' else offense_code_ranks.get(x, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28595035",
   "metadata": {},
   "source": [
    "We also rank victims on each report by the completeness of their demographic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "39cc6e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7747/4226223461.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('report_id').apply(calculate_rank).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "#Create victim rank by demographics completeness\n",
    "def calculate_rank(group):\n",
    "    def inner_calculate_rank(row):\n",
    "        conditions_met = sum(\n",
    "            [\n",
    "                pd.notna(row['victim_age']) and row['victim_age'] != -1,\n",
    "                row['victim_race'] not in [\"Unknown\", None, \"-\"],\n",
    "                row['victim_gender'] not in [\"Unknown\", None, \"-\"],\n",
    "                row['victim_ethnicity'] not in [\"Unknown\", None, \"-\"]\n",
    "            ]\n",
    "        )\n",
    "        return conditions_met\n",
    "     \n",
    "    group['victim_rank'] = group.apply(inner_calculate_rank, axis=1)\n",
    "    #rows with same completeness get same rank\n",
    "    group['victim_rank'] = group['victim_rank'].rank(ascending=False, method='dense').astype(int)\n",
    "    return group\n",
    "\n",
    "# Group the DataFrame by 'report_id' and apply the ranking function to each group.\n",
    "df = df.groupby('report_id').apply(calculate_rank).reset_index(drop=True)\n",
    "\n",
    "# Sort the DataFrame by 'rank' column in ascending order to get the desired ranking.\n",
    "df.sort_values(['report_id','victim_rank'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769570d9",
   "metadata": {},
   "source": [
    "Similarly, we rank suspects on each report by the completeness of their demographic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "aa0b712d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7747/2586714333.py:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('report_id').apply(calculate_rank_s).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "#Create subject rank by demographics completeness\n",
    "def calculate_rank_s(group):\n",
    "    def inner_calculate_rank_s(row):\n",
    "        conditions_met = sum(\n",
    "            [\n",
    "                pd.notna(row['subject_age']) and row['subject_age'] != -1,\n",
    "                row['subject_race'] not in [\"Unknown\", None, \"-\"],\n",
    "                row['subject_gender'] not in [\"Unknown\", None, \"-\"],\n",
    "                row['subject_ethnicity'] not in [\"Unknown\", None, \"-\"]\n",
    "            ]\n",
    "        )\n",
    "        return conditions_met \n",
    "    \n",
    "\n",
    "    group['subject_rank'] = group.apply(inner_calculate_rank_s, axis=1)\n",
    "    #rows with same completeness get same rank\n",
    "    group['subject_rank'] = group['subject_rank'].rank(ascending=False, method='dense').astype(int)\n",
    "    return group\n",
    "\n",
    "# Group the DataFrame by 'report_id' and apply the ranking function to each group.\n",
    "df = df.groupby('report_id').apply(calculate_rank_s).reset_index(drop=True)\n",
    "\n",
    "# Sort the DataFrame by 'rank' column in ascending order to get the desired ranking.\n",
    "df.sort_values(['report_id','subject_rank'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956b4144",
   "metadata": {},
   "source": [
    "Create combined demographics completeness rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "76d9192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create subject + victim rank \n",
    "df['demographics_rank'] = df['victim_rank'] + df['subject_rank']\n",
    "\n",
    "#drop individual ranks (we'll use the combined rank)\n",
    "df = df.drop(['victim_rank', 'subject_rank'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92caab26",
   "metadata": {},
   "source": [
    "### Pivot dataset (keep one row per unique report_id)\n",
    "\n",
    "Note that you will get as many 'victim' columns as the maximum number of victims in a report in your dataset, which might change based on the data sample. This is also the case for suspects and offenses. In our application, we limit the number of offenses, suspects, and victims to 5 each, matching the format of the dummy_narrative_github dataset used in the feature engineering for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "70a05582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame for the final dataset\n",
    "final_data = pd.DataFrame()\n",
    "\n",
    "# Group the original data by 'report_id'\n",
    "grouped = df.groupby('report_id')\n",
    "\n",
    "# Extract common columns for each report\n",
    "final_data['reporting_event_number'] = grouped['reporting_event_number'].first()\n",
    "final_data['report_id'] = grouped['report_id'].first()\n",
    "final_data['precinct'] = grouped['precinct'].first()\n",
    "final_data['beat'] = grouped['beat'].first()\n",
    "final_data['event_start_date'] = grouped['event_start_date'].first()\n",
    "final_data['report_submitted_date'] = grouped['report_submitted_date'].first()\n",
    "final_data['approval_status'] = grouped['approval_status'].first()\n",
    "final_data['report_ucr_approved_by'] = grouped['report_ucr_approved_by'].first()\n",
    "\n",
    "# Extract unique values and labels for each category (offense, victim, subject)\n",
    "for col_prefix in ['offense_id', 'crime_description', 'victim_personid', 'victim_age', 'victim_race', 'victim_gender', 'victim_ethnicity', 'subject_personid', 'subject_age', 'subject_race', 'subject_gender', 'subject_ethnicity', 'offense_rank', 'demographics_rank']:\n",
    "    # Pivot the data to create separate columns for each unique value in the category\n",
    "    # 1) lambda function by group of columns that resets index,2) pivoting, 3) read prefix \n",
    "    pivoted = grouped[col_prefix].apply(lambda x: x.reset_index(drop=True)).unstack().add_prefix(col_prefix + '_')\n",
    "    final_data = pd.concat([final_data, pivoted], axis=1)\n",
    "\n",
    "# Reset the index of the final dataset\n",
    "final_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cc7438",
   "metadata": {},
   "source": [
    "Create flag for reports that have bias-related offense codes to reorder differently. \n",
    "\n",
    "Note that although the reports usually contain an indicator of whether they are events with bias elements or not (such as the offense code), our model does not use these offenses as features since this is precisely the field used for data labeling (see the feature engineering for training process). Instead, the goal of the classifier is to catch reports that might have an incorrect/missing offense code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e87be638",
   "metadata": {},
   "outputs": [],
   "source": [
    "offense_rank_columns = final_data.filter(like='offense_rank')\n",
    "\n",
    "# Check if any value in the 'offense_rank' columns is less than 8, then flag as bias\n",
    "final_data['bias_flag'] = offense_rank_columns.lt(8).any(axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e701a3",
   "metadata": {},
   "source": [
    "### Create separate dataframe for reports with bias events\n",
    "\n",
    "For bias events, we use the offense rank to reorder offenses, associated victims and suspects since we want to ensure that at least the first victim is the victim associated to the highest-ranked offense, usually a bias-related offense (see the ranking above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "87cc58e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate dataframe for bias events\n",
    "bias_reports = final_data[final_data['bias_flag'] == 1]\n",
    "offense_rank_columns = bias_reports.filter(like='offense_rank')\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "result_df_bias = pd.DataFrame()\n",
    "\n",
    "# Reorder columns based on the 'offense_rank' values for flagged rows\n",
    "for index, row in bias_reports.iterrows():\n",
    "    # Get the 'offense_rank' values for the current row\n",
    "    rank_values = row[offense_rank_columns.columns].values\n",
    "    # Get the corresponding suffixes for the 'offense_rank' columns\n",
    "    suffixes = [col.split('_')[-1] for col in offense_rank_columns.columns]\n",
    "    # Get the corresponding 'demographics_rank' values for the current row\n",
    "    demographics_values = [row[f'demographics_rank_{suffix}'] for suffix in suffixes]\n",
    "    \n",
    "    # Sort the columns based on 'offense_rank' x[1] values and, if tied, 'demographics_rank'-x[2]\n",
    "    sorted_columns = sorted(zip(offense_rank_columns.columns, rank_values, demographics_values), \n",
    "                             key=lambda x: (x[1], -x[2]))\n",
    "    \n",
    "    # Create a mapping of old column names to new column names for this row\n",
    "    column_mapping = {}\n",
    "    \n",
    "    for i, (old_name, _, _) in enumerate(sorted_columns, start=1):\n",
    "        new_name = f'offense_rank_{i}'\n",
    "        column_mapping[old_name] = new_name\n",
    "        \n",
    "        # Rename associated columns with the same suffix\n",
    "        suffix = old_name.split('_')[-1]\n",
    "        for column_prefix in ['crime_description', 'offense_id', 'victim_personid', 'victim_age',\n",
    "                              'victim_race', 'victim_gender', 'victim_ethnicity', 'subject_personid',\n",
    "                              'subject_age', 'subject_race', 'subject_gender',\n",
    "                              'subject_ethnicity', 'demographics_rank']:\n",
    "            associated_column = f'{column_prefix}_{suffix}'\n",
    "            new_associated_column = f'{column_prefix}_{i}'\n",
    "            column_mapping[associated_column] = new_associated_column\n",
    "    \n",
    "    # Rename columns for the current row\n",
    "    renamed_row = row.rename(column_mapping)\n",
    "    \n",
    "    # Append the renamed row to the result DataFrame\n",
    "    result_df_bias = pd.concat([result_df_bias, renamed_row.to_frame().T], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d516bc",
   "metadata": {},
   "source": [
    "Get only unique offenses and shift crime description accordingly, which means victims, subjects, and offenses after offense #1 won't necessarily be linked by suffix (e.g., if there are multiple unique offenses, but only one victim, the same victim would be repeated several times, which we don't want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b307a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "offense_rank_columns = bias_reports.filter(like='offense_rank')\n",
    "# Determine the number of columns based on available data\n",
    "num_columns = len(offense_rank_columns.columns)\n",
    "\n",
    "# Iterate over rows in result_df\n",
    "for index, row in result_df_bias.iterrows():\n",
    "    # Create a set to keep track of unique offenses for the current row\n",
    "    unique_offenses = set()\n",
    "    \n",
    "    for i in range(1, num_columns + 1):\n",
    "        col_name = f'offense_id_{i}'\n",
    "        desc_name = f'crime_description_{i}'\n",
    "        \n",
    "        # Check if the offense is already in the set\n",
    "        if row[col_name] in unique_offenses:\n",
    "            # Find the next unique offense not in the set\n",
    "            j = i + 1\n",
    "            while j <= num_columns and (row[col_name] in unique_offenses or row[f'offense_id_{j}'] in unique_offenses):\n",
    "                col_name = f'offense_id_{j}'\n",
    "                desc_name = f'crime_description_{j}'\n",
    "                j += 1\n",
    "        \n",
    "        # Add the unique offense to the set of unique offenses\n",
    "        unique_offenses.add(row[col_name])\n",
    "        \n",
    "        # Update the row with the adjusted offense and description\n",
    "        result_df_bias.at[index, f'offense_id_{i}'] = row[col_name]\n",
    "        result_df_bias.at[index, f'crime_description_{i}'] = row[desc_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a908b8fa",
   "metadata": {},
   "source": [
    "Get only unique victims and shift demographics accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "abedde98",
   "metadata": {},
   "outputs": [],
   "source": [
    "offense_rank_columns = bias_reports.filter(like='offense_rank')\n",
    "# Determine the number of columns based on available data\n",
    "num_columns = len(offense_rank_columns.columns)\n",
    "\n",
    "# Iterate over rows in result_df\n",
    "for index, row in result_df_bias.iterrows():\n",
    "    # Create a set to keep track of unique offenses for the current row\n",
    "    unique_victims = set()\n",
    "    \n",
    "    for i in range(1, num_columns + 1):\n",
    "        col_name = f'victim_personid_{i}'\n",
    "        age_name = f'victim_age_{i}'\n",
    "        race_name = f'victim_race_{i}'\n",
    "        gender_name = f'victim_gender_{i}'\n",
    "        ethnicity_name = f'victim_ethnicity_{i}'\n",
    "        \n",
    "        # Check if the offense is already in the set\n",
    "        if row[col_name] in unique_victims:\n",
    "            # Find the next unique offense not in the set\n",
    "            j = i + 1\n",
    "            while j <= num_columns and (row[col_name] in unique_victims or row[f'victim_personid_{j}'] in unique_victims):\n",
    "                col_name = f'victim_personid_{j}'\n",
    "                age_name = f'victim_age_{j}'\n",
    "                race_name = f'victim_race_{j}'\n",
    "                gender_name = f'victim_gender_{j}'\n",
    "                ethnicity_name = f'victim_ethnicity_{j}'\n",
    "                j += 1\n",
    "        \n",
    "        # Add the unique offense to the set of unique offenses\n",
    "        unique_victims.add(row[col_name])\n",
    "        \n",
    "        # Update the row with the adjusted offense and description\n",
    "        result_df_bias.at[index, f'victim_personid_{i}'] = row[col_name]\n",
    "        result_df_bias.at[index, f'victim_age_{i}'] = row[age_name]\n",
    "        result_df_bias.at[index, f'victim_race_{i}'] = row[race_name]\n",
    "        result_df_bias.at[index, f'victim_gender_{i}'] = row[gender_name]\n",
    "        result_df_bias.at[index, f'victim_ethnicity_{i}'] = row[ethnicity_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefedcc8",
   "metadata": {},
   "source": [
    "Get only unique subjects and shift demographics accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fd40a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "offense_rank_columns = bias_reports.filter(like='offense_rank')\n",
    "# Determine the number of columns based on available data\n",
    "num_columns = len(offense_rank_columns.columns)\n",
    "\n",
    "# Iterate over rows in result_df\n",
    "for index, row in result_df_bias.iterrows():\n",
    "    # Create a set to keep track of unique offenses for the current row\n",
    "    unique_subjects = set()\n",
    "    \n",
    "    for i in range(1, num_columns + 1):\n",
    "        col_name = f'subject_personid_{i}'\n",
    "        age_name = f'subject_age_{i}'\n",
    "        race_name = f'subject_race_{i}'\n",
    "        gender_name = f'subject_gender_{i}'\n",
    "        ethnicity_name = f'subject_ethnicity_{i}'\n",
    "        \n",
    "        # Check if the offense is already in the set\n",
    "        if row[col_name] in unique_subjects:\n",
    "            # Find the next unique offense not in the set\n",
    "            j = i + 1\n",
    "            while j <= num_columns and (row[col_name] in unique_subjects or row[f'subject_personid_{j}'] in unique_subjects):\n",
    "                col_name = f'subject_personid_{j}'\n",
    "                age_name = f'subject_age_{j}'\n",
    "                race_name = f'subject_race_{j}'\n",
    "                gender_name = f'subject_gender_{j}'\n",
    "                ethnicity_name = f'subject_ethnicity_{j}'\n",
    "                j += 1\n",
    "        \n",
    "        # Add the unique offense to the set of unique offenses\n",
    "        unique_subjects.add(row[col_name])\n",
    "        \n",
    "        # Update the row with the adjusted offense and description\n",
    "        result_df_bias.at[index, f'subject_personid_{i}'] = row[col_name]\n",
    "        result_df_bias.at[index, f'subject_age_{i}'] = row[age_name]\n",
    "        result_df_bias.at[index, f'subject_race_{i}'] = row[race_name]\n",
    "        result_df_bias.at[index, f'subject_gender_{i}'] = row[gender_name]\n",
    "        result_df_bias.at[index, f'subject_ethnicity_{i}'] = row[ethnicity_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb83a0a",
   "metadata": {},
   "source": [
    "### Create separate dataframe for reports with no bias-related offenses\n",
    "\n",
    "For no bias-related offenses, we reorder based on offense AND demographics ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "07c004c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate dataframe for bias events\n",
    "no_bias_reports = final_data[final_data['bias_flag'] == 0]\n",
    "offense_rank_columns = no_bias_reports.filter(like='offense_rank')\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Reorder columns based on the 'offense_rank' values for flagged rows\n",
    "for index, row in no_bias_reports.iterrows():\n",
    "    # Get the 'offense_rank' values for the current row\n",
    "    rank_values = row[offense_rank_columns.columns].values\n",
    "    # Get the corresponding suffixes for the 'offense_rank' columns\n",
    "    suffixes = [col.split('_')[-1] for col in offense_rank_columns.columns]\n",
    "    # Get the corresponding 'demographics_rank' values for the current row\n",
    "    demographics_values = [row[f'demographics_rank_{suffix}'] for suffix in suffixes]\n",
    "    \n",
    "    # Sort the columns based on 'offense_rank' x[1] values and, if tied, 'demographics_rank'-x[2]\n",
    "    sorted_columns = sorted(zip(offense_rank_columns.columns, rank_values, demographics_values), \n",
    "                             key=lambda x: (x[1], -x[2]))\n",
    "    \n",
    "    # Create a mapping of old column names to new column names for this row\n",
    "    column_mapping = {}\n",
    "    \n",
    "    for i, (old_name, _, _) in enumerate(sorted_columns, start=1):\n",
    "        new_name = f'offense_rank_{i}'\n",
    "        column_mapping[old_name] = new_name\n",
    "        \n",
    "        # Rename associated columns with the same suffix\n",
    "        suffix = old_name.split('_')[-1]\n",
    "        for column_prefix in ['crime_description', 'offense_id', 'victim_personid', 'victim_age',\n",
    "                              'victim_race', 'victim_gender', 'victim_ethnicity', 'subject_personid',\n",
    "                              'subject_age', 'subject_race', 'subject_gender',\n",
    "                              'subject_ethnicity', 'demographics_rank']:\n",
    "            associated_column = f'{column_prefix}_{suffix}'\n",
    "            new_associated_column = f'{column_prefix}_{i}'\n",
    "            column_mapping[associated_column] = new_associated_column\n",
    "    \n",
    "    # Rename columns for the current row\n",
    "    renamed_row = row.rename(column_mapping)\n",
    "    \n",
    "    # Append the renamed row to the result DataFrame\n",
    "    result_df = pd.concat([result_df, renamed_row.to_frame().T], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc844f9",
   "metadata": {},
   "source": [
    "Get only unique offenses and shift crime description accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7bd2143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_rank_columns = no_bias_reports.filter(like='demographics_rank')\n",
    "# Determine the number of columns based on available data\n",
    "num_columns = len(demographics_rank_columns.columns)\n",
    "\n",
    "# Iterate over rows in result_df\n",
    "for index, row in result_df.iterrows():\n",
    "    # Create a set to keep track of unique offenses for the current row\n",
    "    unique_offenses = set()\n",
    "    \n",
    "    for i in range(1, num_columns + 1):\n",
    "        col_name = f'offense_id_{i}'\n",
    "        desc_name = f'crime_description_{i}'\n",
    "        \n",
    "        # Check if the offense is already in the set\n",
    "        if row[col_name] in unique_offenses:\n",
    "            # Find the next unique offense not in the set\n",
    "            j = i + 1\n",
    "            while j <= num_columns and (row[col_name] in unique_offenses or row[f'offense_id_{j}'] in unique_offenses):\n",
    "                col_name = f'offense_id_{j}'\n",
    "                desc_name = f'crime_description_{j}'\n",
    "                j += 1\n",
    "        \n",
    "        # Add the unique offense to the set of unique offenses\n",
    "        unique_offenses.add(row[col_name])\n",
    "        \n",
    "        # Update the row with the adjusted offense and description\n",
    "        result_df.at[index, f'offense_id_{i}'] = row[col_name]\n",
    "        result_df.at[index, f'crime_description_{i}'] = row[desc_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60139520",
   "metadata": {},
   "source": [
    "Get only unique victims and shift demographics accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0141369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_rank_columns = no_bias_reports.filter(like='demographics_rank')\n",
    "# Determine the number of columns based on available data\n",
    "num_columns = len(demographics_rank_columns.columns)\n",
    "\n",
    "# Iterate over rows in result_df\n",
    "for index, row in result_df.iterrows():\n",
    "    # Create a set to keep track of unique offenses for the current row\n",
    "    unique_victims = set()\n",
    "    \n",
    "    for i in range(1, num_columns + 1):\n",
    "        col_name = f'victim_personid_{i}'\n",
    "        age_name = f'victim_age_{i}'\n",
    "        race_name = f'victim_race_{i}'\n",
    "        gender_name = f'victim_gender_{i}'\n",
    "        ethnicity_name = f'victim_ethnicity_{i}'\n",
    "        \n",
    "        # Check if the offense is already in the set\n",
    "        if row[col_name] in unique_victims:\n",
    "            # Find the next unique offense not in the set\n",
    "            j = i + 1\n",
    "            while j <= num_columns and (row[col_name] in unique_victims or row[f'victim_personid_{j}'] in unique_victims):\n",
    "                col_name = f'victim_personid_{j}'\n",
    "                age_name = f'victim_age_{j}'\n",
    "                race_name = f'victim_race_{j}'\n",
    "                gender_name = f'victim_gender_{j}'\n",
    "                ethnicity_name = f'victim_ethnicity_{j}'\n",
    "                j += 1\n",
    "        \n",
    "        # Add the unique offense to the set of unique offenses\n",
    "        unique_victims.add(row[col_name])\n",
    "        \n",
    "        # Update the row with the adjusted offense and description\n",
    "        result_df.at[index, f'victim_personid_{i}'] = row[col_name]\n",
    "        result_df.at[index, f'victim_age_{i}'] = row[age_name]\n",
    "        result_df.at[index, f'victim_race_{i}'] = row[race_name]\n",
    "        result_df.at[index, f'victim_gender_{i}'] = row[gender_name]\n",
    "        result_df.at[index, f'victim_ethnicity_{i}'] = row[ethnicity_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c9e786",
   "metadata": {},
   "source": [
    "Get only unique subjects and shift demographics accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fea21e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_rank_columns = no_bias_reports.filter(like='demographics_rank')\n",
    "# Determine the number of columns based on available data\n",
    "num_columns = len(demographics_rank_columns.columns)\n",
    "\n",
    "# Iterate over rows in result_df\n",
    "for index, row in result_df.iterrows():\n",
    "    # Create a set to keep track of unique offenses for the current row\n",
    "    unique_subjects = set()\n",
    "    \n",
    "    for i in range(1, num_columns + 1):\n",
    "        col_name = f'subject_personid_{i}'\n",
    "        age_name = f'subject_age_{i}'\n",
    "        race_name = f'subject_race_{i}'\n",
    "        gender_name = f'subject_gender_{i}'\n",
    "        ethnicity_name = f'subject_ethnicity_{i}'\n",
    "        \n",
    "        # Check if the offense is already in the set\n",
    "        if row[col_name] in unique_subjects:\n",
    "            # Find the next unique offense not in the set\n",
    "            j = i + 1\n",
    "            while j <= num_columns and (row[col_name] in unique_subjects or row[f'subject_personid_{j}'] in unique_subjects):\n",
    "                col_name = f'subject_personid_{j}'\n",
    "                age_name = f'subject_age_{j}'\n",
    "                race_name = f'subject_race_{j}'\n",
    "                gender_name = f'subject_gender_{j}'\n",
    "                ethnicity_name = f'subject_ethnicity_{j}'\n",
    "                j += 1\n",
    "        \n",
    "        # Add the unique offense to the set of unique offenses\n",
    "        unique_subjects.add(row[col_name])\n",
    "        \n",
    "        # Update the row with the adjusted offense and description\n",
    "        result_df.at[index, f'subject_personid_{i}'] = row[col_name]\n",
    "        result_df.at[index, f'subject_age_{i}'] = row[age_name]\n",
    "        result_df.at[index, f'subject_race_{i}'] = row[race_name]\n",
    "        result_df.at[index, f'subject_gender_{i}'] = row[gender_name]\n",
    "        result_df.at[index, f'subject_ethnicity_{i}'] = row[ethnicity_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c39d4e7",
   "metadata": {},
   "source": [
    "### Get only the first column of each for both datasets\n",
    "\n",
    "Although the current version of the algorithm uses only the first offense, victim, and subject, we are working on a version of the model that will incorporate more of these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3853ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get only one column of each\n",
    "def one_offense(column_name):\n",
    "    try:\n",
    "        # Extract the numerical part from the column name\n",
    "        numerical_part = int(''.join(filter(str.isdigit, column_name)))\n",
    "        return numerical_part > 1\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "# Get a list of columns to drop\n",
    "columns_to_drop = [col for col in result_df.columns if one_offense(col)]\n",
    "columns_to_drop_b = [col for col in result_df_bias.columns if one_offense(col)]\n",
    "\n",
    "# Drop the columns\n",
    "result_df.drop(columns=columns_to_drop, inplace=True)\n",
    "result_df_bias.drop(columns=columns_to_drop_b, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b98af",
   "metadata": {},
   "source": [
    "### Concatenate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ff180601",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([result_df, result_df_bias], axis=0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329a6fa1",
   "metadata": {},
   "source": [
    "### Read in narratives and merge\n",
    "\n",
    "We store report narratives in a separate dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5c6a888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "narr = pd.read_csv('narratives.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2600c086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merge left on report_id\n",
    "df = final_df.merge(narr[['narrative', 'report_id']], how='left', left_on= 'report_id', right_on= 'report_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e5f755",
   "metadata": {},
   "source": [
    "### NLP Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5144eb8a",
   "metadata": {},
   "source": [
    "Fill missing narratives with neutral word (i.e., narrative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "aa0b8cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['narrative'] = df['narrative'].fillna('narrative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ea89f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the pre-processing functions to the 'narrative' column\n",
    "df['corpus'] = df['narrative'].apply(removeStopWords)\n",
    "df['corpus'] = df['corpus'].apply(removeFeatures)\n",
    "df['corpus'] = df['corpus'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7674664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If empty corpus after processing, replace with 'narrative':\n",
    "df['corpus'] = df['corpus'].replace('','narrative')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f49936a",
   "metadata": {},
   "source": [
    "### One-Hot Encoding Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "74f9231e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7747/1976628151.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['subject_age_1']= df['subject_age_1'].replace(-1, np.NaN)\n",
      "/tmp/ipykernel_7747/1976628151.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['victim_age_1'] = df['victim_age_1'].replace(-1, np.NaN)\n"
     ]
    }
   ],
   "source": [
    "df['subject_age_1']= df['subject_age_1'].replace(-1, np.NaN)\n",
    "df['victim_age_1'] = df['victim_age_1'].replace(-1, np.NaN)\n",
    "\n",
    "df['subject_race_1'] = df['subject_race_1'].replace('-', 'Sub_Race_Unknown')\n",
    "df['subject_race_1'] = df['subject_race_1'].fillna('Sub_Race_Unknown')\n",
    "df['subject_race_1'] = df['subject_race_1'].replace('Unknown', 'Sub_Race_Unknown')\n",
    "\n",
    "df['victim_race_1'] = df['victim_race_1'].replace('-', 'Vic_Race_Unknown')\n",
    "df['victim_race_1'] = df['victim_race_1'].fillna('Vic_Race_Unknown')\n",
    "df['victim_race_1'] = df['victim_race_1'].replace('Unknown', 'Vic_Race_Unknown')\n",
    "\n",
    "df['subject_gender_1'] = df['subject_gender_1'].replace('-', 'Sub_Gender_Unknown')\n",
    "df['subject_gender_1'] = df['subject_gender_1'].fillna('Sub_Gender_Unknown')\n",
    "df['subject_gender_1'] = df['subject_gender_1'].replace('Unknown', 'Sub_Gender_Unknown')\n",
    "\n",
    "df['victim_gender_1'] = df['victim_gender_1'].replace('-', 'Vic_Gender_Unknown')\n",
    "df['victim_gender_1'] = df['victim_gender_1'].fillna('Vic_Gender_Unknown')\n",
    "df['victim_gender_1'] = df['victim_gender_1'].replace('Unknown', 'Vic_Gender_Unknown')\n",
    "\n",
    "df['subject_ethnicity_1'] = df['subject_ethnicity_1'].replace('-', 'Sub_Ethni_Unknown')\n",
    "df['subject_ethnicity_1'] = df['subject_ethnicity_1'].fillna('Sub_Ethni_Unknown')\n",
    "df['subject_ethnicity_1'] = df['subject_ethnicity_1'].replace('Unknown', 'Sub_Ethni_Unknown')\n",
    "\n",
    "df['victim_ethnicity_1'] = df['victim_ethnicity_1'].replace('-', 'Vic_Ethni_Unknown')\n",
    "df['victim_ethnicity_1'] = df['victim_ethnicity_1'].fillna('Vic_Ethni_Unknown')\n",
    "df['victim_ethnicity_1'] = df['victim_ethnicity_1'].replace('Unknown', 'Vic_Ethni_Unknown')\n",
    "\n",
    "df['beat'] = df['beat'].replace('99', 'beat_Unknown')\n",
    "df['beat'] = df['beat'].replace('OOJ', 'beat_OOJ')\n",
    "df['beat'] = df['beat'].replace('Unknown', 'beat_Unknown')\n",
    "df['beat'] = df['beat'].fillna('beat_Unknown')\n",
    "\n",
    "\n",
    "df['precinct'] = df['precinct'].replace('Unknown', 'precinct_Unknown')\n",
    "df['precinct'] = df['precinct'].replace('OOJ', 'precinct_OOJ')\n",
    "df['precinct'] = df['precinct'].fillna('precinct_Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9602e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0716476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "precinct_e = ohe.fit_transform(df[['precinct']])\n",
    "df[ohe.categories_[0]] = precinct_e.toarray()\n",
    "\n",
    "gender_e = ohe.fit_transform(df[['victim_gender_1']])\n",
    "df[ohe.categories_[0]] = gender_e.toarray()\n",
    "\n",
    "race_e = ohe.fit_transform(df[['victim_race_1']])\n",
    "df[ohe.categories_[0]] = race_e.toarray()\n",
    "\n",
    "ethnicity_e = ohe.fit_transform(df[['victim_ethnicity_1']])\n",
    "df[ohe.categories_[0]] = ethnicity_e.toarray()\n",
    "\n",
    "beat_e = ohe.fit_transform(df[['beat']])\n",
    "df[ohe.categories_[0]] = beat_e.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "dcd1cf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new instance of OneHotEncoder for Subject's categories\n",
    "ohe_subject = OneHotEncoder()\n",
    "subject_race_e = ohe_subject.fit_transform(df[['subject_race_1']])\n",
    "\n",
    "# Rename the one-hot encoded columns with a prefix to differentiate them\n",
    "new_column_names = ['subject_' + category for category in ohe_subject.categories_[0]]\n",
    "\n",
    "# Create a DataFrame from the one-hot encoded array and set the column names\n",
    "subject_race_df = pd.DataFrame(subject_race_e.toarray(), columns=new_column_names)\n",
    "\n",
    "# Concatenate the one-hot encoded DataFrame with the original DataFrame\n",
    "df = pd.concat([df, subject_race_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b18c9f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_gender_e = ohe_subject.fit_transform(df[['subject_gender_1']])\n",
    "new_column_names = ['subject_' + category for category in ohe_subject.categories_[0]]\n",
    "# Create a DataFrame from the one-hot encoded array and set the column names\n",
    "subject_gender_df = pd.DataFrame(subject_gender_e.toarray(), columns=new_column_names)\n",
    "\n",
    "# Concatenate the one-hot encoded DataFrame with the original DataFrame\n",
    "df = pd.concat([df, subject_gender_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "88c06bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_ethnicity_e = ohe_subject.fit_transform(df[['subject_ethnicity_1']])\n",
    "new_column_names = ['subject_' + category for category in ohe_subject.categories_[0]]\n",
    "# Create a DataFrame from the one-hot encoded array and set the column names\n",
    "subject_ethnicity_df = pd.DataFrame(subject_ethnicity_e.toarray(), columns=new_column_names)\n",
    "\n",
    "# Concatenate the one-hot encoded DataFrame with the original DataFrame\n",
    "df = pd.concat([df, subject_ethnicity_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6e0888e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECK IF COLUMN IN LIST IS PRESENT, IF NOT, CREATE AND ASSIGN 0 TO IT\n",
    "\n",
    "columns = ['victim_age_1','subject_age_1','East', 'North', 'precinct_OOJ', 'South', 'Southwest', 'West', 'precinct_Unknown', 'Female',\n",
    " 'Gender Diverse (gender non-conforming and/or transgender)', 'Male', 'Vic_Gender_Unknown',\n",
    " 'American Indian or Alaska Native', 'Asian', 'Black or African American', 'Native Hawaiian or Other Pacific Islander',\n",
    " 'Vic_Race_Unknown', 'White', 'Hispanic Or Latino', 'Not Hispanic Or Latino', 'Vic_Ethni_Unknown',\n",
    " 'subject_American Indian or Alaska Native', 'subject_Asian', 'subject_Black or African American',\n",
    " 'subject_Native Hawaiian or Other Pacific Islander', 'subject_Sub_Race_Unknown', 'subject_White',\n",
    " 'subject_Female', 'subject_Gender Diverse (gender non-conforming and/or transgender)',\n",
    " 'subject_Male', 'subject_Sub_Gender_Unknown', 'subject_Hispanic Or Latino', 'subject_Not Hispanic Or Latino',\n",
    " 'subject_Sub_Ethni_Unknown', 'B1', 'B2', 'B3', 'C1', 'C2', 'C3', 'D1', 'D2', 'D3', 'E1', 'E2', 'E3', 'F1', 'F2',\n",
    " 'F3', 'G1', 'G2', 'G3', 'H1', 'H2', 'H3', 'J1', 'J2', 'J3', 'K1', 'K2', 'K3', 'L1', 'L2', 'L3', 'M1', 'M2', 'M3',\n",
    " 'N1', 'N2', 'N3', 'O1', 'O2', 'O3', 'Q1', 'Q2', 'Q3', 'R1', 'R2', 'R3', 'S1', 'S2', 'S3', 'U1', 'U2', 'U3',\n",
    " 'beat_Unknown', 'W1', 'W2', 'W3', 'beat_OOJ']\n",
    "\n",
    "for column in columns:\n",
    "    if column not in df.columns:\n",
    "        df[column] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9c596d",
   "metadata": {},
   "source": [
    "The final_reports.csv file is rewritten everytime this notebook runs. Consider saving the original file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "af3036d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as final reports table (this table is rewritten everytime the code runs)\n",
    "df.to_csv(\"final_reports.csv\", compression='gzip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c2f2da64",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = df[['reporting_event_number','report_id','victim_age_1', 'subject_age_1','corpus', 'East', 'North', 'precinct_OOJ', 'South', 'Southwest', 'West', 'precinct_Unknown', 'Female',\n",
    " 'Gender Diverse (gender non-conforming and/or transgender)', 'Male', 'Vic_Gender_Unknown',\n",
    " 'American Indian or Alaska Native', 'Asian', 'Black or African American', 'Native Hawaiian or Other Pacific Islander',\n",
    " 'Vic_Race_Unknown', 'White', 'Hispanic Or Latino', 'Not Hispanic Or Latino', 'Vic_Ethni_Unknown',\n",
    " 'subject_American Indian or Alaska Native', 'subject_Asian', 'subject_Black or African American',\n",
    " 'subject_Native Hawaiian or Other Pacific Islander', 'subject_Sub_Race_Unknown', 'subject_White',\n",
    " 'subject_Female', 'subject_Gender Diverse (gender non-conforming and/or transgender)',\n",
    " 'subject_Male', 'subject_Sub_Gender_Unknown', 'subject_Hispanic Or Latino', 'subject_Not Hispanic Or Latino',\n",
    " 'subject_Sub_Ethni_Unknown', 'B1', 'B2', 'B3', 'C1', 'C2', 'C3', 'D1', 'D2', 'D3', 'E1', 'E2', 'E3', 'F1', 'F2',\n",
    " 'F3', 'G1', 'G2', 'G3', 'H1', 'H2', 'H3', 'J1', 'J2', 'J3', 'K1', 'K2', 'K3', 'L1', 'L2', 'L3', 'M1', 'M2', 'M3',\n",
    " 'N1', 'N2', 'N3', 'O1', 'O2', 'O3', 'Q1', 'Q2', 'Q3', 'R1', 'R2', 'R3', 'S1', 'S2', 'S3', 'U1', 'U2', 'U3',\n",
    " 'beat_Unknown', 'W1', 'W2', 'W3', 'beat_OOJ']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e781c1",
   "metadata": {},
   "source": [
    "### NLP: Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "52bc4005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in trained vectorizer from training feature engineering\n",
    "import joblib\n",
    "\n",
    "cv = joblib.load('countvectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fc0d4521",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = cv.transform(final_features['corpus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "19a62724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep processed reporting_event_number and report_ids\n",
    "rens = final_features['reporting_event_number']\n",
    "r_ids = final_features['report_id']\n",
    "\n",
    "#to avoid error with hstack, make all columns numeric\n",
    "final_features = final_features.drop(['corpus', 'reporting_event_number', 'report_id'], axis=1).apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0cc84e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge sparse matrix back with other features\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "final_df = hstack([vectorized_corpus, final_features.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "eef02c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2dd84029",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = pd.DataFrame.sparse.from_spmatrix(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f750a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read rens and r_ids\n",
    "final_features['reporting_event_number'] = rens\n",
    "final_features['report_id'] = r_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "438a66d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['victim_age_1', 'subject_age_1','East', 'North', 'precinct_OOJ', 'South', 'Southwest', 'West',\n",
    " 'precinct_Unknown', 'Female', 'Gender Diverse (gender non-conforming and/or transgender)', 'Male', 'Vic_Gender_Unknown',\n",
    " 'American Indian or Alaska Native', 'Asian', 'Black or African American', 'Native Hawaiian or Other Pacific Islander',\n",
    " 'Vic_Race_Unknown', 'White', 'Hispanic Or Latino', 'Not Hispanic Or Latino', 'Vic_Ethni_Unknown',\n",
    " 'subject_American Indian or Alaska Native', 'subject_Asian', 'subject_Black or African American',\n",
    " 'subject_Native Hawaiian or Other Pacific Islander', 'subject_Sub_Race_Unknown', 'subject_White',\n",
    " 'subject_Female', 'subject_Gender Diverse (gender non-conforming and/or transgender)',\n",
    " 'subject_Male', 'subject_Sub_Gender_Unknown', 'subject_Hispanic Or Latino', 'subject_Not Hispanic Or Latino',\n",
    " 'subject_Sub_Ethni_Unknown', 'B1', 'B2', 'B3', 'C1', 'C2', 'C3', 'D1', 'D2', 'D3', 'E1', 'E2', 'E3', 'F1', 'F2',\n",
    " 'F3', 'G1', 'G2', 'G3', 'H1', 'H2', 'H3', 'J1', 'J2', 'J3', 'K1', 'K2', 'K3', 'L1', 'L2', 'L3', 'M1', 'M2', 'M3',\n",
    " 'N1', 'N2', 'N3', 'O1', 'O2', 'O3', 'Q1', 'Q2', 'Q3', 'R1', 'R2', 'R3', 'S1', 'S2', 'S3', 'U1', 'U2', 'U3',\n",
    " 'beat_Unknown', 'W1', 'W2', 'W3', 'beat_OOJ', 'reporting_event_number','report_id']\n",
    "\n",
    "col_names = np.concatenate((words, names))\n",
    "\n",
    "final_features = final_features.rename(columns=dict(zip(final_features.columns, col_names), inplace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ef8ffa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save report_id and reporting_event_number to identify positive predictions for tasks\n",
    "final_features[['report_id', 'reporting_event_number']].to_csv('inference_reports.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "de03de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save without report_id and no index or header for xgboost\n",
    "final_features = final_features.drop(['report_id', 'reporting_event_number'], axis = 1)\n",
    "final_features.to_csv('inference_features.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
