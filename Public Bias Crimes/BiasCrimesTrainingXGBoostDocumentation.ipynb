{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c057e013",
   "metadata": {},
   "source": [
    "# Bias Events - XGBoost Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0573631e",
   "metadata": {},
   "source": [
    "## Source Data and Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b4167c",
   "metadata": {},
   "source": [
    "- train_final.csv (created in training_feature_engineering)\n",
    "- test_final.csv (created in training_feature_engineering)\n",
    "- countvectorizer.pkl (trained vectorizer from training_feature_engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6918a686",
   "metadata": {},
   "source": [
    "## Classification with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f5d4f6",
   "metadata": {},
   "source": [
    "SPD's fully productionized classification pipeline uses __[AWS' XGBoost built-in algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html)__. The data input of this algorithm requires 'rows representing observations, one column representing the target variable or label, and the remaining columns representing features.' If using columnar input and .csv format, the first column must be the label and the input should not have headers. \n",
    "\n",
    "We use asynchronous training and __[hyperparameter tuning jobs](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex-tuning-job.html)__ in AWS Sagemaker. We specifically implement __[Bayesian Optimization Hyperparameter Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html)__.\n",
    "\n",
    "For the sake of replication we show how to train your model locally using __[DMLC XGBoost](https://xgboost.readthedocs.io/en/stable/index.html)__ and __[scikit-optimize BayesSearchCV](https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html)__.\n",
    "\n",
    "We heavily borrow the tuning implementation code from __[this tutorial](https://towardsdatascience.com/binary-classification-xgboost-hyperparameter-tuning-scenarios-by-non-exhaustive-grid-search-and-c261f4ce098d)__.\n",
    "\n",
    "Our application's tuning and training strategies were selected based on the amount of data we work with and other data and computational efficiency considerations. The sizes of the example datasets are relatively small, creating issues with overfitting, hyperparameter sensitivity to subsamples and high variance, unreliable performance metrics, as well as poor generalization, when applying the same strategies we use on SPD's larger datasets. \n",
    "\n",
    "Again, we provide this code for demonstration purposes. Careful consideration should be given to your application's specific conditions and applicable best practices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19c5283",
   "metadata": {},
   "source": [
    "### *Class Imbalance*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee490463",
   "metadata": {},
   "source": [
    "Although not a problem with the example data, SPD's training dataset is highly imbalanced given that events with bias elements account for a very small portion of overall crime. We therefore train, tune hyperparameters, and select a classification threshold that optimize the F-1 score, to prioritize the correct classification of the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c060c9",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "- Trained XGBoost model object used for inference.\n",
    "- Optimal classification threshold used for inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
